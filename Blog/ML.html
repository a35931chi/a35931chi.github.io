<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	<title>Anthony DS - Machine Learning Reference</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<style class="anchorjs"></style><link rel="stylesheet" href="Attachments/bootstrap.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<link href="Attachments/ekko-lightbox.css" rel="stylesheet">

	<!-- for documentation only -->
	<style type="text/css">
		div.row > div > div.row {
			margin-bottom: 15px;
		}

		html {
			background-color: #02709e;
		}
		body {
			background: #fefefe;
			padding-bottom: 50px;
		}

		div.top-container {
			padding-top:100px;
			background-color: #02709e;
			color:#ccc;
		}
		div.top-container h1 {
			color:white;
		}
		div.top-container a {
			color:#ccc;
			border-bottom:1px dotted white;
		}
		div.top-container a:hover {
			color: white;
			cursor:pointer;
			border-bottom:1px solid white;
			text-decoration: none;
		}
		div.top-header {
			margin-bottom:100px;
		}

		h2 {

			display:inline-block;
			margin-top:20px;
			margin-bottom:20px;
		}

		h3 {
			padding-bottom:5px;
			margin-bottom:10px;
			border-bottom:1px solid #f2f2f2;
			margin-top: 50px;
		}

		h4:not(.modal-title) {
			margin-top:40px;
		}
		
		img {margin-top: 4px;}
		
		figure {
			position: relative;
		}

		figure figcaption {
			font-size: 22px;
			color: #fff;
			text-decoration: none;
			bottom: 10px;
			right: 20px;
			position: absolute;
			background-color: #000;
		}
		code[data-code], code.block-code {
			display:block;
			overflow:scroll;
			font-size:12px;
			white-space: pre;
		}

		table {
			font-size:12px;
		}
		.footer {
			text-align: center;
		}
		.footer span {
			margin-top:100px;
			font-size:12px;
			background-color:#02709e;
			color:white;
			display:inline-block;
			padding:6px;
		}
		.footer span a {
			color:#ccc;
			border-bottom:1px dotted white;
		}
		.footer span a:hover {
			cursor:pointer;
			color: white;
			border-bottom:1px solid white;
			text-decoration: none;
		}
		a.anchorjs-link {
			color:black;
		}
		a.anchorjs-link:hover {
			color:#02709e;
			text-decoration: none;
		}
		#floatme {float: left;}
		
		.row {
			display: -ms-flexbox; /* IE 10 */
			display: flex;
			-ms-flex-wrap: wrap; /* IE 10 */
			flex-wrap: wrap;
			padding: 0 4px;
		}

		/* Create two equal columns that sits next to each other */
		.column {
			-ms-flex: 50%; /* IE 10 */
			flex: 50%;
			padding: 0 4px;
		}
		.column img {
			margin-top: 8px;
			vertical-align: middle;
		}
		
	</style>
    </head>
    <body style="" class="">
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
			
			<a name="top" style = "margin-left: 20px;"><h2><b>Machine Learning Reference</b></h2></a>
			<br>
			
		<div class="row General" style="max-width:1000px">
			<div class="col-12">
				<a href="Attachments/ML/sklearn_ml_map.png" data-toggle="lightbox" data-gallery="ML" data-footer="scikit-learn algorithm cheat-sheet">
				<img style = "padding: 0 15pd; float: left" class = 'myImg' src="Attachments/ML/sklearn_ml_map.png" height="300" width=auto, align = 'right', hspace = "20"></a>
				<p style = "margin-top: 5px;">Most of you have seen this image from <a href ="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">scikit-learn.org</a>. This may even be your bible in determining which ML algo to try, or even utilize. My attempt here is to provide summaries, pros and cons, and criteria for some of the popular machine learning algos, in addition to those that are available in the scikit learn library. I'm hoping this will help you with your ML journey. </p>
				
				<br>
				<ul>
					<a href="#Regression"><h5>Regression</h5></a>
					<a href="#Classification"><h5>Classification</h5></a>
				</ul>
			</div>		

		</div>	
		<div class="row General">
			<div class="col-12">
			<a name="Regression"><h4><b>Regression</b></h4></a>
				<table class="table table-bordered">
				  <tr>
					<th>Algorithm</th>
					<th>Criteria</th>
					<th>Overview</th>
					<th>Pros & Cons</th>
					<th>Tips</th>
					<th>Best Application</th>
				  </tr>
				  <tr>
					<td><b>SGD Regressor</b></td>
					<td>SGD Regressor is well suited for regression problems with a large number of training samples (10k+, preferably 100k+ )</td>
					<td>
						<p>Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.</p><p>SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.</p>
					</td>
					<td>Pros:
						<ul>
							<li>Efficiency</li>
							<li>Ease of implementation (lots of opportunities for code tuning)</li>
						</ul>
						Cons:
						<ul>
							<li>SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations</li>
							<li>SGD is sensitive to feature scaling</li>
						</ul>
					</td>
					<td>
						<ul>
							<li>Sensitive to feature scaling, highly recommended to scale your data</li>
							<li>Finding a reasonable regularization term \alpha is best done using GridSearchCV, usually in the range 10.0**-np.arange(1,7)</li>
							<li>Empirically, SGD converges after observing approx. 10^6 training samples. A reasonable first guess for the number of iterations is n_iter = np.ceil(10**6 / n), where n is the size of the training set.</li>
							<li>PCA features: often wise to scale the feature values by some constant c such that the average L2 norm of the training data equals one</li>
							<li>Averaged SGD works best with a larger number of features and a higher eta0</li>
						</ul>
					</td>
					<td>-</td>
				  </tr>
				  <tr>
					<td><b>Ridge Regressor</b></td>
					<td></td>
					<td>Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares. Effectively, Ridge penalize data with outliers, or features with outliers.</td>
					<td>Pros:
						<ul>
							<li>reduce effects of outliers</li>
							<li>no data distribution requirement</li>
						</ul></td>
					<td></td>
					<td>If we want to reduce effects of outliers.</td>
				  </tr>
				  
				  <tr>
					<td><b>Lasso Regressor</b></td>
					<td></td>
					<td>Lasso is short for least absolute shrinkage and selector operator. Lasso regression was similar to an ordinary least square regression, where the algorithm was trying to minimize the squared error term. Where the Lasso was different from OLS was the λ term. When λ increased, many of the correlated coefficients would be reduced to zeros. This property was known as feature selection, where Lasso regression only keep features that are uncorrelated and relevant. However, due to feature selection, we may lose information contained in the dataset. Due to Lasso’s simplicity, efficiency, and ability to perform feature selection, Lasso could also be a good benchmark for future regression exercises.</td>
					<td>Pros:
						<ul>
							<li>Better than usual methods of automatic variable selection</li>
							<li>no data distribution requirement</li>
						</ul>
						Cons:
						<ul>
							<li>Garbage in, Garbage out</li>
							<li>Ignores variables that are insignificant, but might be interesting</li>
						</ul></td>
					<td>It is generally used when we have more number of features, because it automatically does feature selection.</td>
					<td>Use it when we have a lot of features that might be correlated.</td>
				  </tr>
				  <tr>
					<td><b>ElasticNet</b></td>
					<td></td>
					<td>ElasticNet is a combination of Ridge and Lasso algorithms. Ridge regression also contained a λ term, where if λ increased, the error term penalties were also magnified, thereby discouraging overfitting. For ElasticNet, we needed to define Alpha and L1_ratio, where Alpha =  λ1 + λ2, and L1_ratio = λ1 / (λ1 + λ2). Note that if L1_ratio = 1, then we have the Lasso algorithm; if L1 = 0, then we have the Ridge algorithm. Therefore, ElasticNet is a combination of give and take between Lasso (feature selection), and Ridge (regularization). Due to ElasticNet’s simplicity, efficiency and ability to perform feature selection and regularization, ElasticNet could also bebe a good benchmark for future regression exercise. </td>
					<td>Pros:
						<ul>
							<li>Can tune both L1 and L2 penalty terms</li>
							<li>inherit some of Ridge’s stability under rotation</li>
							<li>no data distribution requirement</li>
						</ul>
					</td>
					<td>When trying OLS, Lasso or Ridge, whynot include ElasticNet?</td>
					<td></td>
				  </tr>
				  
				  <tr>
					<td><b>SVR</b></td>
					<td>If there's not enough data, we can try SVR</td>
					<td>A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are more commonly used in classification problems and as such, this is what we will focus on in this post.<br>SVMs are based on the idea of finding a hyperplane that best divides a dataset into classes.</td>
					<td>Pros:
						<ul>
							<li>Effective in high dimensional spaces.</li>
							<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
							<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
							<li>Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>
							<li>Accuracy</li>
							<li>Works well on smaller cleaner datasets</li>
							<li>It can be more efficient because it uses a subset of training points</li>
							<li>no data distribution requirement</li>
						</ul>
						Cons:
						<ul>
							<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
							<li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).</li>
							<li>Isn’t suited to larger datasets as the training time with SVMs can be high</li>
							<li>Less effective on noisier datasets with overlapping classes</li>

						</ul></td>
					<td>Try both kernel = 'linera' or 'rbf' to get best results</td>
					<td>SVM is used for text classification tasks such as category assignment, detecting spam and sentiment analysis. It is also commonly used for image recognition challenges, performing particularly well in aspect-based recognition and color-based classification. SVM also plays a vital role in many areas of handwritten digit recognition, such as postal automation services.</td>
				  </tr>
				</table> 
				<a href="#top">back to the top</a>
				 
				<a name="Classification"><h4><b>Classification</b></h4></a>
				<table class="table table-bordered">
				   <tr>
					<td><b>SGD Regressor</b></td>
					<td>SGD Regressor is well suited for regression problems with a large number of training samples (10k+, preferably 100k+ )</td>
					<td>
						<p>Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.</p><p>SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.</p>
					</td>
					<td>Pros:
						<ul>
							<li>Efficiency</li>
							<li>Ease of implementation (lots of opportunities for code tuning)</li>
						</ul>
						Cons:
						<ul>
							<li>SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations</li>
							<li>SGD is sensitive to feature scaling</li>
						</ul>
					</td>
					<td>
						<ul>
							<li>Sensitive to feature scaling, highly recommended to scale your data</li>
							<li>Finding a reasonable regularization term \alpha is best done using GridSearchCV, usually in the range 10.0**-np.arange(1,7)</li>
							<li>Empirically, SGD converges after observing approx. 10^6 training samples. A reasonable first guess for the number of iterations is n_iter = np.ceil(10**6 / n), where n is the size of the training set.</li>
							<li>PCA features: often wise to scale the feature values by some constant c such that the average L2 norm of the training data equals one</li>
							<li>Averaged SGD works best with a larger number of features and a higher eta0</li>
						</ul>
					</td>
					<td>-</td>
				  </tr>
				  
				  <tr>
					<td><b>Kernel Approximation</b></td>
					<td>If SDG Classifier doesn't work, and we have data of 100k+, we can try out Kernel Approximation.</td>
					<td>-</td>
					<td>-</td>
					<td>-</td>
					<td>-</td>
				  </tr>
				  
				  <tr>
					<td><b>SVC</b></td>
					<td>If there's not enough data, we can try SVC</td>
					<td>A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are more commonly used in classification problems and as such, this is what we will focus on in this post.<br>SVMs are based on the idea of finding a hyperplane that best divides a dataset into classes.</td>
					<td>Pros:
						<ul>
							<li>Effective in high dimensional spaces.</li>
							<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
							<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
							<li>Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>
							<li>Accuracy</li>
							<li>Works well on smaller cleaner datasets</li>
							<li>It can be more efficient because it uses a subset of training points</li>
							<li>no data distribution requirement</li>
						</ul>
						Cons:
						<ul>
							<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
							<li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).</li>
							<li>Isn’t suited to larger datasets as the training time with SVMs can be high</li>
							<li>Less effective on noisier datasets with overlapping classes</li>

						</ul></td>
					<td>Try both kernel = 'linera' or 'rbf' to get best results</td>
					<td>SVM is used for text classification tasks such as category assignment, detecting spam and sentiment analysis. It is also commonly used for image recognition challenges, performing particularly well in aspect-based recognition and color-based classification. SVM also plays a vital role in many areas of handwritten digit recognition, such as postal automation services.</td>
				  </tr>
				  <tr>
					<td><b>Decision Tree</b></td>
					<td></td>
					<td>Decision trees are simple but intuitive models that utilize a top-down approach in which the root node creates binary splits until a certain criteria is met. This binary splitting of nodes provides a predicted value based on the interior nodes leading to the terminal (final) nodes. In a classification context, a decision tree will output a predicted target class for each terminal node produced. This is done by assessing the entropy and information gain at each split. </td>
					<td>Pros:
						<ul>
							<li>Easy to interpret and make for straightforward visualizations.</li>
							<li>The internal workings are capable of being observed and thus make it possible to reproduce work</li>
							<li>Can handle both numerical and categorical data.</li>
							<li>Perform well on large datasets</li>
							<li>Fast</li>
							<li>good for few categories variables</li>
							<li>don't suffer multicollinearity</li>
							<li>no data distribution requirement</li>
						</ul>
						Cons:
						<ul>
							<li>Depending on the algo, it doesn't always yield the global optimum</li>
							<li>Prone to overfitting (Decision trees tend to have high variance when they utilize different training and test sets of the same data)</li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>
				  <tr>
					<td><b>KNN (K Nearest Neighbor)</b></td>
					<td>KNN is a supervised algo. Given some training vectors, KNN identifies the k nearest neighbors of a datapoint, and in turn identifies/votes on the data point's classification. When k = 1, each training vector defineds a region in space, defining a Voronoi partition of the space.</td>
					<td>Doesn't need a lot of data</td>
					<td>Pros:
						<ul>
							<li>Simple to implement </li>
							<li>Flexible to feature / distance choices</li>
							<li>Naturally handles multi-class cases </li>
							<li>Can do well in practice with enough representative data </li>
							
						</ul>
						Cons:
						<ul>
							<li>complexity of searching the nearest neighbors for each sample, and storage of data</li>
							<li>Must know we have a meaningful distance function</li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>
				  <tr>
					<td><b>K-Means</b></td>
					<td><p>K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:<br>1. The centroids of the K clusters, which can be used to label new data<br>2. Labels for the training data (each data point is assigned to a single cluster)<br>Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically.<br>Each centroid of a cluster is a collection of feature values which define the resulting groups. Examining the centroid feature weights can be used to qualitatively interpret what kind of group each cluster represents.</p>
					</td>
					<td></td>
					<td>Pros:
						<ul>
							<li>pretty fast</li>
						</ul>
						Cons:
						<ul>
							<li>have to know how many groups/classes there are</li>
							<li>may yield different clustering results on different runs of the algorithm (results may not be repeatable)</li>
						</ul></td>
					<td></td>
					<td>Some Business Cases:<br>
						Behavioral segmentation:
						<ul>
							<li>Segment by purchase history</li>
							<li>Segment by activities on application, website, or platform</li>
							<li>Define personas based on interests</li>
							<li>Create profiles based on activity monitoring</li>
							<li></li>
						</ul>
						Inventory categorization:
						<ul>
							<li>Group inventory by sales activity</li>
							<li>Group inventory by manufacturing metrics</li>
						</ul>
						Sorting sensor measurements:
						<ul>
							<li>Detect activity types in motion sensors</li>
							<li>Group images</li>
							<li>Separate audio</li>
							<li>Identify groups in health monitoring</li>
						</ul>
						Detecting bots or anomalies:
						<ul>
							<li>Separate valid activity groups from bots</li>
							<li>Group valid activity to clean up outlier detection</li>
						</ul>
					</td>
				  </tr>
				  <tr>
					<td><b>Random Forest</b></td>
					<td>Exploding number of observations</td>
					<td>Through a process known as bootstrap aggregating (or bagging), it’s possible to create an ensemble (forest) of trees where multiple training sets are generated with replacement, meaning data instances can be repeated. This approach helps reduce variance by averaging the ensemble's results, creating a majority-votes model. Another important feature of bagging trees is that the resulting model uses the entire feature space when considering node splits. Bagging trees allow the trees to grow without pruning, reducing the tree-depth sizes and resulting in high variance but lower bias, which can help improve predictive power. Random Forests train each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data. The general concept is that you divide your data into several portions, use a relatively weak classifier/regressor to process, and then combine them. Random forest is flexible and can enhance the accuracy/performance of the weak algorithm.</td>
					<td>Pros:
						<ul>
							<li>Generalizes well, less overfitting</li>
							<li>parallel or distributed computing</li>
							<li>Almost always have lower classification error and better f-scores than decision trees and SVMs. Easy to explain.</li>
							<li>Deal really well with uneven data sets that have missing variables</li>
							<li>Give you a really good idea of which features in your data set are the most important for free</li>
							<li>Generally train faster than SVMs</li>
						</ul>
						Cons:
						<ul>
							<li>Can be omputationally Expensive</li>
						</ul></td>
					<td>https://www.datascience.com/resources/notebooks/random-forest-intro</td>
					<td>Random Forest can be used for a plethora of data circumstances including, but not limited to:
						<ul>
							<li>Image classification</li>
							<li>Detecting fraudulent cases in banking systems</li>
							<li>Recommendation engines</li>
							<li>Feature selection</li>
						</ul>
					</td>
				  </tr>
				  <tr>
					<td><b>Ada (Adaptive) Boost</b></td>
					<td></td>
					<td>AdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more more weight, whereas easy to predict instances are given less weight.<br>Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence.<br>After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on the training data.<br>Because so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed</td>
					<td>Pros:
						<ul>
							<li></li>
							<li></li>
						</ul>
						Cons:
						<ul>
							<li></li>
							<li></li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>
				  <tr>
					<td><b>Gradient Boost</b></td>
					<td></td>
					<td>GBTs build trees one at a time, where each new tree helps to correct errors made by previously trained tree. With each tree added, the model becomes even more expressive. There are typically three parameters - number of trees, depth of trees and learning rate, and the each tree built is generally shallow</td>
					<td>Pros:
						<ul>
							<li></li>
							<li></li>
						</ul>
						Cons:
						<ul>
							<li>prone to overfitting</li>
							<li>GBDTs will usually perform better than RF, but they are harder to get right. More concretely, GBDTs have more hyper-parameters to tune and are also more prone to overfitting. RFs can almost work "out of the box" and that is one reason why they are very popular.</li>
							<li>GBDT training generally takes longer because of the fact that trees are built sequentially</li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>
				  
				  
				  <tr>
					<td><b>XG Boost</b></td>
					<td></td>
					<td>XGBoost stood for extreme gradient boosting. It was an algorithm based on Gradient Boosting, which combines weak learners into a single strong learner in an iterative fashion. In Gradient Boosting, Gradient descent was use in each iteration to minimize the error function (an additive process). In the case for the Random Forest algorithm, it would generate independent trees with different results, which were aggregated to form the recommendation for the ensemble algorithm. Contrasting Random Forest to Gradient Boosting, instead of having independent trees, trees were added to the previous tree to compliment what the previous tree failed to realize. XGBoost was an algorithm developed to include regularization, which placed emphasis to control for over-fitting. In addition to building a more generalized model, XGBoost also had system optimization and algorithm improvement, which could run more efficiently. As an algorithm that won many kaggle competitions, I expected XGBoost algorithm to perform well. </td>
					<td>Pros:
						<ul>
							<li></li>
							<li></li>
						</ul>
						Cons:
						<ul>
							<li></li>
							<li></li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>
				  <tr>
					<td><b>Neural Network</b></td>
					<td></td>
					<td></td>
					<td>Pros:
						<ul>
							<li>good to model the non-linear data with large number of input features</li>
							<li>widely used in industry</li>
							<li>many open source implementations</li>
							<li>good in image classification, video, audio, text. </li>
						</ul>
						Cons:
						<ul>
							<li>only for numerical inputs, vectors with constant number of values, and datasets with non-missing data.</li>
							<li>The classification boundaries are hard to understand intuitively and ANNs are computationally expensive.</li>
							<li>black box, makes them difficult to work with, it’s like trying interrogate the human unconscious for the reasons behind our conscious actions.</li>
							<li>difficult to train: the training outcome can be nondeterministic and depend crucially on the choice of initial parameters</li>
							<li>t makes them difficult to troubleshoot when they don't work as you expect, and when they do work, you will never really feel confident that they will generalize well to data not included in your training set because, fundamentally, you don't understand how your network is solving the problem</li>
							<li>multi-layer neural networks are usually hard to train, and require tuning lots of parameters</li>
							<li>Neural networks are not probabilistic, unlike their more statistical or Bayesian counterparts. A neural network might give you a continuous number as its output (e.g. a score) but translating that into a probability is often difficult. Approaches with stronger theoretical foundations usually give you those probabilities directly.</li>
							<li>not a general-purpose technique for classification. </li>
						</ul></td>
					<td></td>
					<td></td>
				  </tr>

				</table>
				<a href="#top">back to the top</a>
			</div>
		</div>
            
		<script src="Attachments/jquery-3.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
		<script src="Attachments/popper.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
		<script src="Attachments/bootstrap.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
		<script src="Attachments/ekko-lightbox.js"></script>

		<!-- for documentation only -->
		<script src="Attachments/anchor.js"></script>
		<script type="text/javascript">
			$(document).ready(function ($) {
				// delegate calls to data-toggle="lightbox"
				$(document).on('click', '[data-toggle="lightbox"]:not([data-gallery="navigateTo"]):not([data-gallery="example-gallery-11"])', function(event) {
					event.preventDefault();
					return $(this).ekkoLightbox({
						onShown: function() {
							if (window.console) {
								return console.log('Checking our the events huh?');
							}
						},
						onNavigate: function(direction, itemIndex) {
							if (window.console) {
								return console.log('Navigating '+direction+'. Current item: '+itemIndex);
							}
						}
					});
				});

				// disable wrapping
				$(document).on('click', '[data-toggle="lightbox"][data-gallery="example-gallery-11"]', function(event) {
					event.preventDefault();
					return $(this).ekkoLightbox({
						wrapping: false
					});
				});

				//Programmatically call
				$('#open-image').click(function (e) {
					e.preventDefault();
					$(this).ekkoLightbox();
				});
				$('#open-youtube').click(function (e) {
					e.preventDefault();
					$(this).ekkoLightbox();
				});

				// navigateTo
				$(document).on('click', '[data-toggle="lightbox"][data-gallery="navigateTo"]', function(event) {
					event.preventDefault();

					return $(this).ekkoLightbox({
						onShown: function() {

							this.modal().on('click', '.modal-footer a', function(e) {

								e.preventDefault();
								this.navigateTo(2);

							}.bind(this));

						}
					});
				});


				/**
				 * Documentation specific - ignore this
				 */
				anchors.options.placement = 'left';
				anchors.add('h3');
				$('code[data-code]').each(function() {

					var $code = $(this),
						$pair = $('div[data-code="'+$code.data('code')+'"]');

					$code.hide();
					var text = $code.text($pair.html()).html().trim().split("\n");
					var indentLength = text[text.length - 1].match(/^\s+/)
					indentLength = indentLength ? indentLength[0].length : 24;
					var indent = '';
					for(var i = 0; i < indentLength; i++)
						indent += ' ';
					if($code.data('trim') == 'all') {
						for (var i = 0; i < text.length; i++)
							text[i] = text[i].trim();
					} else  {
						for (var i = 0; i < text.length; i++)
							text[i] = text[i].replace(indent, '    ').replace('    ', '');
					}
					text = text.join("\n");
					$code.html(text).show();

				});
			});
	</script>
    

</body></html>